# ABJ-Attack

## Table of Contents

- [Overview](#overview)
- [Argument Specification](#argument-specification)
- [Quick Start](#quick-start)



## Overview

This repository shares the code of our latest work on LLMs jailbreaking. In this work:
    
- We identify and explore a novel jailbreak attack paradigm that shifts from input-level to reasoning-level manipulation, uncovering an overlooked attack surface in the reasoning process of aligned language models.
- We propose Analyzing-based Jailbreak (ABJ), a black-box attack that manipulates modelâ€™s multi-step reasoning process to induce harmful outputs. ABJ introduces a unified multimodal framework that enables reasoning chain manipulation in both LLMs and VLMs, revealing alignment vulnerabilities across modalities.
- We conduct extensive experiments on state-of-the-art LLMs, VLMs, and RLMs, showing that ABJ outperforms baselines in attack effectiveness, transferability, and efficiency. Additionally, we analyze key factors contributing to its effectiveness and discuss potential defense strategies.




## Argument Specification
  
- `target_model`: The name of target model.

- `assist_model`: The name of assist model.

- `judge_model`: The name of judge model.
  
- `max_attack_rounds`: Number of attack iteration rounds, default is `3`.

- `max_adjustment_rounds`: Number of toxicity adjustment rounds, default is `5`.

- `target_model_cuda_id`: Number of the GPU for target model, default is `cuda:0`.

- `assist_model_cuda_id`: Number of the GPU for assist model, default is `cuda:1`.

- `judge_model_cuda_id`: Number of the GPU for judge model, default is `cuda:2`.

  
## Quick Start

Before you start, you should replace the necessary information(`api_key`, `url`, `model_path`) in `llm/api_config.py` and `llm/llm_model.py`.

1. Build enviroment:

   ```sh
   cd ABJ-Attack
   conda create -n ABJ python==3.11
   conda activate ABJ
   pip install -r requirements.txt
   ```

2. Run ABJ-Attack:

     ```sh
     python ABJ.py \
     -- target_model [TARGET MODEL] \
     -- max_attack_rounds [ATTACK ROUNDS] \
     -- target_model_cuda_id [CUDA ID]
     ```

    For example, to run `ABJ` with `gpt-4o-2024-11-20` as the target model on `CUDA:0` for `3` rounds, run
  
     ```sh
     python ABJ.py \
     -- target_model gpt4o \
     -- max_attack_rounds 3 \
     -- target_model_cuda_id cuda:1
     ```
